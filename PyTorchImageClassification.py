""" Increased regularization from 0.0003 to 0.0005
Epoch 1, Training Loss: 0.14302, Validation Loss: 0.13351, Validation Accuracy: 0.06
Epoch 2, Training Loss: 0.12671, Validation Loss: 0.11663, Validation Accuracy: 0.09
Epoch 3, Training Loss: 0.11905, Validation Loss: 0.11201, Validation Accuracy: 0.11
Epoch 4, Training Loss: 0.11303, Validation Loss: 0.10887, Validation Accuracy: 0.13
Epoch 5, Training Loss: 0.10856, Validation Loss: 0.10628, Validation Accuracy: 0.17
Epoch 6, Training Loss: 0.10703, Validation Loss: 0.10532, Validation Accuracy: 0.16
Epoch 7, Training Loss: 0.10377, Validation Loss: 0.10344, Validation Accuracy: 0.17
Epoch 8, Training Loss: 0.10018, Validation Loss: 0.10192, Validation Accuracy: 0.18
Epoch 9, Training Loss: 0.09936, Validation Loss: 0.09990, Validation Accuracy: 0.19
Epoch 10, Training Loss: 0.09708, Validation Loss: 0.09947, Validation Accuracy: 0.20
Epoch 11, Training Loss: 0.09513, Validation Loss: 0.09715, Validation Accuracy: 0.21
Epoch 12, Training Loss: 0.09124, Validation Loss: 0.09583, Validation Accuracy: 0.23
Epoch 13, Training Loss: 0.09258, Validation Loss: 0.10092, Validation Accuracy: 0.19
Epoch 14, Training Loss: 0.08865, Validation Loss: 0.09225, Validation Accuracy: 0.26
Epoch 15, Training Loss: 0.08857, Validation Loss: 0.09501, Validation Accuracy: 0.23
Epoch 16, Training Loss: 0.08731, Validation Loss: 0.09159, Validation Accuracy: 0.27
Epoch 17, Training Loss: 0.08369, Validation Loss: 0.09005, Validation Accuracy: 0.28
Epoch 18, Training Loss: 0.08353, Validation Loss: 0.09318, Validation Accuracy: 0.25
Epoch 19, Training Loss: 0.08404, Validation Loss: 0.08985, Validation Accuracy: 0.27
Epoch 20, Training Loss: 0.07963, Validation Loss: 0.08738, Validation Accuracy: 0.27
Epoch 21, Training Loss: 0.07748, Validation Loss: 0.08746, Validation Accuracy: 0.27
Epoch 22, Training Loss: 0.07588, Validation Loss: 0.08802, Validation Accuracy: 0.29
Epoch 23, Training Loss: 0.07654, Validation Loss: 0.08974, Validation Accuracy: 0.29
Epoch 24, Training Loss: 0.07451, Validation Loss: 0.08767, Validation Accuracy: 0.28
Epoch 25, Training Loss: 0.07238, Validation Loss: 0.08823, Validation Accuracy: 0.31
Epoch 26, Training Loss: 0.07249, Validation Loss: 0.09013, Validation Accuracy: 0.28
Epoch 27, Training Loss: 0.06535, Validation Loss: 0.08123, Validation Accuracy: 0.35
Epoch 28, Training Loss: 0.06295, Validation Loss: 0.08073, Validation Accuracy: 0.36
Epoch 29, Training Loss: 0.06247, Validation Loss: 0.07930, Validation Accuracy: 0.36
Epoch 30, Training Loss: 0.05974, Validation Loss: 0.07945, Validation Accuracy: 0.36
Epoch 31, Training Loss: 0.06036, Validation Loss: 0.08148, Validation Accuracy: 0.37
Epoch 32, Training Loss: 0.05887, Validation Loss: 0.07934, Validation Accuracy: 0.38
Epoch 33, Training Loss: 0.05725, Validation Loss: 0.07955, Validation Accuracy: 0.38
Epoch 34, Training Loss: 0.05587, Validation Loss: 0.07863, Validation Accuracy: 0.38
Epoch 35, Training Loss: 0.05646, Validation Loss: 0.07806, Validation Accuracy: 0.39
Epoch 36, Training Loss: 0.05586, Validation Loss: 0.07677, Validation Accuracy: 0.38
Epoch 37, Training Loss: 0.05295, Validation Loss: 0.07795, Validation Accuracy: 0.40
Epoch 38, Training Loss: 0.05436, Validation Loss: 0.07870, Validation Accuracy: 0.39
Epoch 39, Training Loss: 0.05498, Validation Loss: 0.08045, Validation Accuracy: 0.38
Epoch 40, Training Loss: 0.05156, Validation Loss: 0.07833, Validation Accuracy: 0.40
Epoch 41, Training Loss: 0.04972, Validation Loss: 0.07842, Validation Accuracy: 0.38
Epoch 42, Training Loss: 0.04959, Validation Loss: 0.07560, Validation Accuracy: 0.41
Epoch 43, Training Loss: 0.04952, Validation Loss: 0.07680, Validation Accuracy: 0.41
Epoch 44, Training Loss: 0.04730, Validation Loss: 0.07865, Validation Accuracy: 0.40
Epoch 45, Training Loss: 0.04856, Validation Loss: 0.07993, Validation Accuracy: 0.38
Epoch 46, Training Loss: 0.04608, Validation Loss: 0.07300, Validation Accuracy: 0.43
Epoch 47, Training Loss: 0.04471, Validation Loss: 0.07327, Validation Accuracy: 0.45
Epoch 48, Training Loss: 0.04530, Validation Loss: 0.07418, Validation Accuracy: 0.43
Epoch 49, Training Loss: 0.04282, Validation Loss: 0.07316, Validation Accuracy: 0.43
Epoch 50, Training Loss: 0.04377, Validation Loss: 0.07661, Validation Accuracy: 0.43
Epoch 51, Training Loss: 0.04246, Validation Loss: 0.07367, Validation Accuracy: 0.42
Epoch 52, Training Loss: 0.04287, Validation Loss: 0.07533, Validation Accuracy: 0.43
Epoch 53, Training Loss: 0.04056, Validation Loss: 0.07054, Validation Accuracy: 0.46
Epoch 54, Training Loss: 0.03597, Validation Loss: 0.07082, Validation Accuracy: 0.47
Epoch 55, Training Loss: 0.03540, Validation Loss: 0.07005, Validation Accuracy: 0.48
Epoch 56, Training Loss: 0.03497, Validation Loss: 0.06953, Validation Accuracy: 0.48
Epoch 57, Training Loss: 0.03333, Validation Loss: 0.07079, Validation Accuracy: 0.46
Epoch 58, Training Loss: 0.03493, Validation Loss: 0.07020, Validation Accuracy: 0.48
Epoch 59, Training Loss: 0.03308, Validation Loss: 0.07088, Validation Accuracy: 0.47
Epoch 60, Training Loss: 0.03177, Validation Loss: 0.06990, Validation Accuracy: 0.49
Epoch 61, Training Loss: 0.03211, Validation Loss: 0.06995, Validation Accuracy: 0.47
Epoch 62, Training Loss: 0.03209, Validation Loss: 0.07242, Validation Accuracy: 0.46
Epoch 63, Training Loss: 0.03262, Validation Loss: 0.06887, Validation Accuracy: 0.49
Epoch 64, Training Loss: 0.02995, Validation Loss: 0.06755, Validation Accuracy: 0.50
Epoch 65, Training Loss: 0.02856, Validation Loss: 0.06790, Validation Accuracy: 0.50
Epoch 66, Training Loss: 0.02792, Validation Loss: 0.06806, Validation Accuracy: 0.51
Epoch 67, Training Loss: 0.02878, Validation Loss: 0.06791, Validation Accuracy: 0.49
Epoch 68, Training Loss: 0.02823, Validation Loss: 0.06748, Validation Accuracy: 0.51
Epoch 69, Training Loss: 0.02848, Validation Loss: 0.06793, Validation Accuracy: 0.51
Epoch 70, Training Loss: 0.02762, Validation Loss: 0.06775, Validation Accuracy: 0.51
Epoch 71, Training Loss: 0.02822, Validation Loss: 0.06864, Validation Accuracy: 0.49
Epoch 72, Training Loss: 0.02593, Validation Loss: 0.06865, Validation Accuracy: 0.50
Epoch 73, Training Loss: 0.02630, Validation Loss: 0.06755, Validation Accuracy: 0.51
Epoch 74, Training Loss: 0.02636, Validation Loss: 0.06836, Validation Accuracy: 0.51
Epoch 75, Training Loss: 0.02590, Validation Loss: 0.06667, Validation Accuracy: 0.51
Epoch 76, Training Loss: 0.02575, Validation Loss: 0.06706, Validation Accuracy: 0.51
Epoch 77, Training Loss: 0.02485, Validation Loss: 0.06765, Validation Accuracy: 0.50
Epoch 78, Training Loss: 0.02467, Validation Loss: 0.06666, Validation Accuracy: 0.52
Epoch 79, Training Loss: 0.02412, Validation Loss: 0.06754, Validation Accuracy: 0.51
Epoch 80, Training Loss: 0.02382, Validation Loss: 0.06760, Validation Accuracy: 0.51
Epoch 81, Training Loss: 0.02389, Validation Loss: 0.06710, Validation Accuracy: 0.51
Epoch 82, Training Loss: 0.02398, Validation Loss: 0.06708, Validation Accuracy: 0.51
Epoch 83, Training Loss: 0.02396, Validation Loss: 0.06647, Validation Accuracy: 0.52
Epoch 84, Training Loss: 0.02288, Validation Loss: 0.06672, Validation Accuracy: 0.51
Epoch 85, Training Loss: 0.02370, Validation Loss: 0.06736, Validation Accuracy: 0.51
Epoch 86, Training Loss: 0.02383, Validation Loss: 0.06700, Validation Accuracy: 0.51
Epoch 87, Training Loss: 0.02477, Validation Loss: 0.06701, Validation Accuracy: 0.52
Epoch 88, Training Loss: 0.02308, Validation Loss: 0.06680, Validation Accuracy: 0.52
Epoch 89, Training Loss: 0.02337, Validation Loss: 0.06663, Validation Accuracy: 0.52
Epoch 90, Training Loss: 0.02261, Validation Loss: 0.06654, Validation Accuracy: 0.52
Epoch 91, Training Loss: 0.02192, Validation Loss: 0.06681, Validation Accuracy: 0.51
Epoch 92, Training Loss: 0.02449, Validation Loss: 0.06619, Validation Accuracy: 0.52
Epoch 93, Training Loss: 0.02312, Validation Loss: 0.06675, Validation Accuracy: 0.52
Epoch 94, Training Loss: 0.02279, Validation Loss: 0.06655, Validation Accuracy: 0.52
Epoch 95, Training Loss: 0.02179, Validation Loss: 0.06681, Validation Accuracy: 0.51
Epoch 96, Training Loss: 0.02299, Validation Loss: 0.06669, Validation Accuracy: 0.51
Epoch 97, Training Loss: 0.02198, Validation Loss: 0.06643, Validation Accuracy: 0.52
Epoch 98, Training Loss: 0.02199, Validation Loss: 0.06612, Validation Accuracy: 0.52
Epoch 99, Training Loss: 0.02270, Validation Loss: 0.06633, Validation Accuracy: 0.52
Epoch 100, Training Loss: 0.02206, Validation Loss: 0.06647, Validation Accuracy: 0.52
Test Accuracy: 0.47
"""


import torch  # Needed for the neural networks
import scipy  # Needed to process the downloads from torchvision
import torch.nn as nn  # Needed for the neural networks
from torch import save, load  # Needed to save/load the pt file.
import torch.optim as optim  # Needed to optimise the neural network
from torch.optim.lr_scheduler import StepLR
from torch.optim.lr_scheduler import ReduceLROnPlateau
from torchvision import datasets, transforms  # Needed to download and transform/process the images

# Define the data transformations, this way all the images have the same size.
# This allows us to run the images through the same neural network.
# Otherwise, the input layer would need to change.
train_transform = transforms.Compose([
    transforms.RandomResizedCrop(256, scale=(0.8, 1.0)),
    transforms.RandomHorizontalFlip(),
    transforms.RandomRotation(15),
    transforms.ColorJitter(brightness=0.1, contrast=0.1, saturation=0.1, hue=0.05),
    transforms.RandomAffine(degrees=0, translate=(0.05, 0.05), scale=(0.95, 1.05), shear=5),
    transforms.ToTensor(),
    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
])

test_transform = transforms.Compose([
    transforms.Resize((256, 256)),
    transforms.ToTensor(),
    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
])

# Load the Flowers-102 dataset from torchvision.
# Torchvision already has the Oxford Flowers-102 dataset in it, so all we have to do is download it.
train_dataset = datasets.Flowers102(root='./data', split='train', transform=train_transform, download=True)
val_dataset = datasets.Flowers102(root='./data', split='val', transform=test_transform, download=True)
test_dataset = datasets.Flowers102(root='./data', split='test', transform=test_transform, download=True)

# Create data loaders. These allow you to load the data for the neural network. The batch size allows you to train
# the model in batches rather than per image, allowing better and more complex learning. We shuffle the training set
# so that in each epoch, the neural network is opposed to different orders of images, strengthening the hidden layers.
train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=32, shuffle=True, num_workers=4)
val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=32, shuffle=False, num_workers=4)
test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=32, shuffle=False, num_workers=4)

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

class FlowerClassifier(nn.Module):
    def __init__(self, num_classes):
        super(FlowerClassifier, self).__init__()
        self.conv_block1 = nn.Sequential(
            nn.Conv2d(3, 32, kernel_size=3, stride=1, padding=1),
            nn.BatchNorm2d(32),
            nn.ReLU(),
            nn.MaxPool2d(kernel_size=2, stride=2)
        )
        self.conv_block2 = nn.Sequential(
            nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1),
            nn.BatchNorm2d(64),
            nn.ReLU(),
            nn.MaxPool2d(kernel_size=2, stride=2)
        )
        self.conv_block3 = nn.Sequential(
            nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1),
            nn.BatchNorm2d(128),
            nn.ReLU(),
            nn.MaxPool2d(kernel_size=2, stride=2)
        )
        self.conv_block4 = nn.Sequential(
            nn.Conv2d(128, 256, kernel_size=3, stride=1, padding=1),
            nn.BatchNorm2d(256),
            nn.ReLU(),
            nn.MaxPool2d(kernel_size=2, stride=2)
        )
        self.conv_block5 = nn.Sequential(
            nn.Conv2d(256, 512, kernel_size=3, stride=1, padding=1),
            nn.BatchNorm2d(512),
            nn.ReLU(),
            nn.MaxPool2d(kernel_size=2, stride=2)
        )
        self.conv_block6 = nn.Sequential(
            nn.Conv2d(512, 512, kernel_size=3, stride=1, padding=1),
            nn.BatchNorm2d(512),
            nn.ReLU(),
            nn.MaxPool2d(kernel_size=2, stride=2)
        )
        self.global_avg_pool = nn.AdaptiveAvgPool2d((1, 1))
        self.fc = nn.Sequential(
            nn.Linear(512, 1024),
            nn.ReLU(),
            nn.Dropout(0.5),
            nn.Linear(1024, num_classes)
        )

    def forward(self, x):
        x = self.conv_block1(x)
        x = self.conv_block2(x)
        x = self.conv_block3(x)
        x = self.conv_block4(x)
        x = self.conv_block5(x)
        x = self.conv_block6(x)
        x = self.global_avg_pool(x)
        x = x.view(x.size(0), -1)
        x = self.fc(x)
        return x

# Training setup remains unchanged



# Initialize the model, loss function, and optimizer.
# The number of classes is 102, as there are 102 different types of flowers, hence the name Flowers-102.
model = FlowerClassifier(num_classes=102).to(device)

# We will be using Cross Entropy Loss. There are many reasons for this, for example cross entropy loss encourages the
# model to assign high probabilities to the correct class and low probabilities to the incorrect classes. It is also
# very common in multi-class classification neural networks. It focuses on the overall correctness of the model
# rather than focusing on small details which is important for a dataset this size and most importantly,
# it works very well with the optimising algorithm we will be using, stochastic gradient descent.
criterion = nn.CrossEntropyLoss().to(device)

# The optimiser we will be using is stochastic gradient descent. One of the main reasons why I used stochastic
# gradient descent is because we've done gradient descent in our practical. Also, SGD processes small batches at a time,
# making it computationally efficient. It reaches conclusions relatively faster than other optimising algorithms and
# the randomness allows for easier generation of more complex algorithms rather than a linear convergence.

# Adam
optimizer = optim.Adam(model.parameters(), lr=0.0005, weight_decay=0.0005)

# ReduceLROnPlateau
scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=5)

best_val_loss = float('inf')
counter = 0
best_model_state = None

for epoch in range(100):  # increase to 50 epochs
    model.train()
    train_loss = 0
    for images, labels in train_loader:
        images, labels = images.to(device), labels.to(device)
        optimizer.zero_grad()
        outputs = model(images)
        loss = criterion(outputs, labels)
        loss.backward()
        optimizer.step()
        train_loss += loss.item()

    val_loss = 0
    val_accuracy = 0
    model.eval()
    with torch.no_grad():
        for images, labels in val_loader:
            images, labels = images.to(device), labels.to(device)
            outputs = model(images)
            val_loss += criterion(outputs, labels).item()
            _, predicted = torch.max(outputs.data, 1)
            val_accuracy += (predicted == labels).sum().item()

    val_loss /= len(val_loader.dataset)
    val_accuracy /= len(val_loader.dataset)
    scheduler.step(val_loss)  # Scheduler steps based on validation loss

    print(f'Epoch {epoch + 1}, Training Loss: {train_loss / len(train_loader.dataset):.5f}, Validation Loss: {val_loss:.5f}, Validation Accuracy: {val_accuracy:.2f}')

    if val_loss < best_val_loss:
        best_val_loss = val_loss
        best_model_state = model.state_dict()  # Save the best model's state dictionary
        counter = 0
    else:
        counter += 1
        if counter >= 10:
            print(f"Early stopping at epoch {epoch+1}")
            break

# Save the trained model
# torch.save(model.state_dict(), 'flower_classifier.pt')

model.load_state_dict(best_model_state)

# Evaluation on the test set
model.eval()  # Set the model to evaluation mode
accuracy = 0.0
with torch.no_grad():  # Disable gradient computation
    for images, labels in test_loader:
        images, labels = images.to(device), labels.to(device)
        outputs = model(images)  # Forward pass
        _, predicted = torch.max(outputs.data, 1)  # Get the predicted classes
        accuracy += (predicted == labels).sum().item()  # Calculate the accuracy

accuracy /= len(test_dataset)
print(f'Test Accuracy: {accuracy:.2f}')