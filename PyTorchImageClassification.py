""" Decreased smoothing factor to 0.1 - best training run
Epoch 1, Training Loss: 0.27981, Validation Loss: 0.25044, Validation Accuracy: 0.11
Epoch 2, Training Loss: 0.24942, Validation Loss: 0.22025, Validation Accuracy: 0.14
Epoch 3, Training Loss: 0.23140, Validation Loss: 0.20637, Validation Accuracy: 0.19
Epoch 4, Training Loss: 0.22204, Validation Loss: 0.20278, Validation Accuracy: 0.19
Epoch 5, Training Loss: 0.20735, Validation Loss: 0.18962, Validation Accuracy: 0.23
Epoch 6, Training Loss: 0.19799, Validation Loss: 0.17776, Validation Accuracy: 0.30
Epoch 7, Training Loss: 0.18943, Validation Loss: 0.17408, Validation Accuracy: 0.29
Epoch 8, Training Loss: 0.18143, Validation Loss: 0.16447, Validation Accuracy: 0.33
Epoch 9, Training Loss: 0.17320, Validation Loss: 0.16572, Validation Accuracy: 0.33
Epoch 10, Training Loss: 0.16258, Validation Loss: 0.15310, Validation Accuracy: 0.37
Epoch 11, Training Loss: 0.16086, Validation Loss: 0.16185, Validation Accuracy: 0.36
Epoch 12, Training Loss: 0.15137, Validation Loss: 0.15628, Validation Accuracy: 0.34
Epoch 13, Training Loss: 0.14821, Validation Loss: 0.14786, Validation Accuracy: 0.39
Epoch 14, Training Loss: 0.14378, Validation Loss: 0.14018, Validation Accuracy: 0.43
Epoch 15, Training Loss: 0.13654, Validation Loss: 0.13737, Validation Accuracy: 0.45
Epoch 16, Training Loss: 0.13365, Validation Loss: 0.13311, Validation Accuracy: 0.44
Epoch 17, Training Loss: 0.12830, Validation Loss: 0.13046, Validation Accuracy: 0.47
Epoch 18, Training Loss: 0.12459, Validation Loss: 0.13255, Validation Accuracy: 0.46
Epoch 19, Training Loss: 0.11954, Validation Loss: 0.12812, Validation Accuracy: 0.49
Epoch 20, Training Loss: 0.11585, Validation Loss: 0.13612, Validation Accuracy: 0.45
Epoch 21, Training Loss: 0.11488, Validation Loss: 0.12261, Validation Accuracy: 0.49
Epoch 22, Training Loss: 0.10810, Validation Loss: 0.12783, Validation Accuracy: 0.48
Epoch 23, Training Loss: 0.10528, Validation Loss: 0.11796, Validation Accuracy: 0.52
Epoch 24, Training Loss: 0.10174, Validation Loss: 0.12109, Validation Accuracy: 0.53
Epoch 25, Training Loss: 0.10088, Validation Loss: 0.12165, Validation Accuracy: 0.50
Epoch 26, Training Loss: 0.09856, Validation Loss: 0.12183, Validation Accuracy: 0.51
Epoch 27, Training Loss: 0.09600, Validation Loss: 0.11217, Validation Accuracy: 0.55
Epoch 28, Training Loss: 0.09243, Validation Loss: 0.11262, Validation Accuracy: 0.55
Epoch 29, Training Loss: 0.09176, Validation Loss: 0.12104, Validation Accuracy: 0.51
Epoch 30, Training Loss: 0.08891, Validation Loss: 0.11675, Validation Accuracy: 0.52
Epoch 31, Training Loss: 0.08648, Validation Loss: 0.11251, Validation Accuracy: 0.54
Epoch 32, Training Loss: 0.08496, Validation Loss: 0.10984, Validation Accuracy: 0.58
Epoch 33, Training Loss: 0.08412, Validation Loss: 0.12189, Validation Accuracy: 0.50
Epoch 34, Training Loss: 0.08404, Validation Loss: 0.10855, Validation Accuracy: 0.58
Epoch 35, Training Loss: 0.08388, Validation Loss: 0.11593, Validation Accuracy: 0.55
Epoch 36, Training Loss: 0.08113, Validation Loss: 0.11405, Validation Accuracy: 0.56
Epoch 37, Training Loss: 0.08294, Validation Loss: 0.11075, Validation Accuracy: 0.55
Epoch 38, Training Loss: 0.07747, Validation Loss: 0.11203, Validation Accuracy: 0.55
Epoch 39, Training Loss: 0.07768, Validation Loss: 0.10638, Validation Accuracy: 0.57
Epoch 40, Training Loss: 0.07609, Validation Loss: 0.11351, Validation Accuracy: 0.56
Epoch 41, Training Loss: 0.07430, Validation Loss: 0.10630, Validation Accuracy: 0.59
Epoch 42, Training Loss: 0.07470, Validation Loss: 0.10855, Validation Accuracy: 0.58
Epoch 43, Training Loss: 0.07370, Validation Loss: 0.10162, Validation Accuracy: 0.61
Epoch 44, Training Loss: 0.07281, Validation Loss: 0.10020, Validation Accuracy: 0.58
Epoch 45, Training Loss: 0.07402, Validation Loss: 0.11594, Validation Accuracy: 0.55
Epoch 46, Training Loss: 0.07396, Validation Loss: 0.09927, Validation Accuracy: 0.61
Epoch 47, Training Loss: 0.07363, Validation Loss: 0.10014, Validation Accuracy: 0.62
Epoch 48, Training Loss: 0.07312, Validation Loss: 0.10468, Validation Accuracy: 0.58
Epoch 49, Training Loss: 0.07239, Validation Loss: 0.10462, Validation Accuracy: 0.58
Epoch 50, Training Loss: 0.07147, Validation Loss: 0.10872, Validation Accuracy: 0.61
Epoch 51, Training Loss: 0.07047, Validation Loss: 0.10735, Validation Accuracy: 0.59
Epoch 52, Training Loss: 0.07036, Validation Loss: 0.09693, Validation Accuracy: 0.65
Epoch 53, Training Loss: 0.06893, Validation Loss: 0.10458, Validation Accuracy: 0.59
Epoch 54, Training Loss: 0.06898, Validation Loss: 0.10990, Validation Accuracy: 0.55
Epoch 55, Training Loss: 0.06957, Validation Loss: 0.10621, Validation Accuracy: 0.59
Epoch 56, Training Loss: 0.06997, Validation Loss: 0.10795, Validation Accuracy: 0.57
Epoch 57, Training Loss: 0.06899, Validation Loss: 0.11095, Validation Accuracy: 0.56
Epoch 58, Training Loss: 0.06928, Validation Loss: 0.11432, Validation Accuracy: 0.56
Epoch 59, Training Loss: 0.06606, Validation Loss: 0.08915, Validation Accuracy: 0.67
Epoch 60, Training Loss: 0.06474, Validation Loss: 0.09016, Validation Accuracy: 0.66
Epoch 61, Training Loss: 0.06378, Validation Loss: 0.09310, Validation Accuracy: 0.67
Epoch 62, Training Loss: 0.06455, Validation Loss: 0.09359, Validation Accuracy: 0.65
Epoch 63, Training Loss: 0.06347, Validation Loss: 0.09392, Validation Accuracy: 0.67
Epoch 64, Training Loss: 0.06343, Validation Loss: 0.09402, Validation Accuracy: 0.68
Epoch 65, Training Loss: 0.06237, Validation Loss: 0.09610, Validation Accuracy: 0.65
Epoch 66, Training Loss: 0.06140, Validation Loss: 0.08862, Validation Accuracy: 0.69
Epoch 67, Training Loss: 0.06035, Validation Loss: 0.08906, Validation Accuracy: 0.69
Epoch 68, Training Loss: 0.06019, Validation Loss: 0.09072, Validation Accuracy: 0.69
Epoch 69, Training Loss: 0.05991, Validation Loss: 0.09146, Validation Accuracy: 0.69
Epoch 70, Training Loss: 0.05998, Validation Loss: 0.09413, Validation Accuracy: 0.66
Epoch 71, Training Loss: 0.05990, Validation Loss: 0.09606, Validation Accuracy: 0.66
Epoch 72, Training Loss: 0.06070, Validation Loss: 0.09054, Validation Accuracy: 0.68
Epoch 73, Training Loss: 0.06026, Validation Loss: 0.08816, Validation Accuracy: 0.71
Epoch 74, Training Loss: 0.05902, Validation Loss: 0.08844, Validation Accuracy: 0.70
Epoch 75, Training Loss: 0.05857, Validation Loss: 0.08855, Validation Accuracy: 0.72
Epoch 76, Training Loss: 0.05816, Validation Loss: 0.09031, Validation Accuracy: 0.70
Epoch 77, Training Loss: 0.05857, Validation Loss: 0.09218, Validation Accuracy: 0.68
Epoch 78, Training Loss: 0.05893, Validation Loss: 0.08811, Validation Accuracy: 0.71
Epoch 79, Training Loss: 0.05951, Validation Loss: 0.08998, Validation Accuracy: 0.69
Epoch 80, Training Loss: 0.05887, Validation Loss: 0.08966, Validation Accuracy: 0.71
Epoch 81, Training Loss: 0.05891, Validation Loss: 0.09233, Validation Accuracy: 0.69
Epoch 82, Training Loss: 0.05869, Validation Loss: 0.09246, Validation Accuracy: 0.68
Epoch 83, Training Loss: 0.05911, Validation Loss: 0.09278, Validation Accuracy: 0.67
Epoch 84, Training Loss: 0.05885, Validation Loss: 0.09149, Validation Accuracy: 0.70
Epoch 85, Training Loss: 0.05848, Validation Loss: 0.08817, Validation Accuracy: 0.71
Epoch 86, Training Loss: 0.05722, Validation Loss: 0.09122, Validation Accuracy: 0.69
Epoch 87, Training Loss: 0.05735, Validation Loss: 0.08487, Validation Accuracy: 0.73
Epoch 88, Training Loss: 0.05729, Validation Loss: 0.09250, Validation Accuracy: 0.70
Epoch 89, Training Loss: 0.05692, Validation Loss: 0.08867, Validation Accuracy: 0.72
Epoch 90, Training Loss: 0.05762, Validation Loss: 0.09219, Validation Accuracy: 0.69
Epoch 91, Training Loss: 0.05715, Validation Loss: 0.08512, Validation Accuracy: 0.75
Epoch 92, Training Loss: 0.05685, Validation Loss: 0.08806, Validation Accuracy: 0.73
Epoch 93, Training Loss: 0.05746, Validation Loss: 0.09110, Validation Accuracy: 0.70
Epoch 94, Training Loss: 0.05710, Validation Loss: 0.09014, Validation Accuracy: 0.72
Epoch 95, Training Loss: 0.05644, Validation Loss: 0.08481, Validation Accuracy: 0.74
Epoch 96, Training Loss: 0.05640, Validation Loss: 0.08699, Validation Accuracy: 0.73
Epoch 97, Training Loss: 0.05647, Validation Loss: 0.08681, Validation Accuracy: 0.74
Epoch 98, Training Loss: 0.05640, Validation Loss: 0.08822, Validation Accuracy: 0.72
Epoch 99, Training Loss: 0.05634, Validation Loss: 0.09049, Validation Accuracy: 0.72
Epoch 100, Training Loss: 0.05693, Validation Loss: 0.09215, Validation Accuracy: 0.72
Epoch 101, Training Loss: 0.05637, Validation Loss: 0.08788, Validation Accuracy: 0.75
Epoch 102, Training Loss: 0.05631, Validation Loss: 0.08846, Validation Accuracy: 0.73
Epoch 103, Training Loss: 0.05582, Validation Loss: 0.08507, Validation Accuracy: 0.75
Epoch 104, Training Loss: 0.05561, Validation Loss: 0.08772, Validation Accuracy: 0.74
Epoch 105, Training Loss: 0.05573, Validation Loss: 0.08742, Validation Accuracy: 0.74
Epoch 106, Training Loss: 0.05572, Validation Loss: 0.08495, Validation Accuracy: 0.76
Epoch 107, Training Loss: 0.05580, Validation Loss: 0.08645, Validation Accuracy: 0.73
Epoch 108, Training Loss: 0.05527, Validation Loss: 0.08667, Validation Accuracy: 0.76
Epoch 109, Training Loss: 0.05518, Validation Loss: 0.08795, Validation Accuracy: 0.74
Epoch 110, Training Loss: 0.05527, Validation Loss: 0.08764, Validation Accuracy: 0.75
Early stopping at epoch 110
Test Accuracy: 0.72
"""

import torch  # Needed for the neural networks
import scipy  # Needed to process the downloads from torchvision
import torch.nn as nn  # Needed for the neural networks
from torch import save, load  # Needed to save/load the pt file.
import torch.optim as optim  # Needed to optimise the neural network
from torch.optim.lr_scheduler import ReduceLROnPlateau  # The scheduler used to change learning rate
from torchvision import datasets, transforms  # Needed to download and transform/process the images

# Define the data transformations, this way all the images have the same size. Certain data augmentation techniques
# like random horizontal flips, rotations and color jitters used to make the data more complex. This allows us to run
# the images through the same neural network. Otherwise, the input layer would need to change.
train_transform = transforms.Compose([
    transforms.RandomResizedCrop(256, scale=(0.75, 1.0)),
    transforms.RandomHorizontalFlip(),
    transforms.RandomRotation(20),
    transforms.ColorJitter(brightness=0.1, contrast=0.1, saturation=0.1, hue=0.05),
    transforms.ToTensor(),
    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
])

test_transform = transforms.Compose([
    transforms.Resize((256, 256)),
    transforms.ToTensor(),
    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
])

# Load the Flowers-102 dataset from torchvision.
# Torchvision already has the Oxford Flowers-102 dataset in it, so all we have to do is download it.
train_dataset = datasets.Flowers102(root='./data', split='train', transform=train_transform, download=True)
val_dataset = datasets.Flowers102(root='./data', split='val', transform=test_transform, download=True)
test_dataset = datasets.Flowers102(root='./data', split='test', transform=test_transform, download=True)

# Create data loaders. These allow you to load the data for the neural network. The batch size allows you to train
# the model in batches rather than per image, allowing better and more complex learning. We shuffle the training set
# so that in each epoch, the neural network is opposed to different orders of images, strengthening the hidden layers.
# Num workers added to introduce parallel learning - can cause "harmless" warnings.
train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=16, shuffle=True, num_workers=4)
val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=16, shuffle=False, num_workers=4)
test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=16, shuffle=False, num_workers=4)

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")  # We send everything to the gpu if it exists.


# Convolutional neural networks outperform practically all other types of neural networks in image classification.
# One of the main reasons for this is that convolutional neural networks don't only mix and match nodes but rather
# are more prone to catching patterns.
class FlowerClassifier(nn.Module):
    def __init__(self, num_classes):
        super(FlowerClassifier, self).__init__()
        self.conv_block1 = nn.Sequential(  # First convolutional block.
            nn.Conv2d(3, 32, kernel_size=3, stride=1, padding=1),
            nn.BatchNorm2d(32),  # Batch normalization filter applied to catch complex patterns.
            nn.ReLU(),  # Rectified linear unit used to introduce nonlinearity.
            nn.MaxPool2d(kernel_size=2, stride=2)  # Pooling used before moving onto the next block.
        )
        self.conv_block2 = nn.Sequential(  # Second convolutional block.
            nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1),
            nn.BatchNorm2d(64),
            nn.ReLU(),
            nn.MaxPool2d(2, 2)
        )
        self.conv_block3 = nn.Sequential(  # Third convolutional block.
            nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1),
            nn.BatchNorm2d(128),
            nn.ReLU(),
            nn.MaxPool2d(2, 2)
        )
        self.conv_block4 = nn.Sequential(  # Fourth convolutional block.
            nn.Conv2d(128, 256, kernel_size=3, stride=1, padding=1),
            nn.BatchNorm2d(256),
            nn.ReLU(),
            nn.MaxPool2d(2, 2)
        )
        self.conv_block5 = nn.Sequential(  # Fifth convolutional block.
            nn.Conv2d(256, 512, kernel_size=3, stride=1, padding=1),
            nn.BatchNorm2d(512),
            nn.ReLU(),
            nn.MaxPool2d(2, 2)
        )
        self.conv_block6 = nn.Sequential(  # Sixth convolutional block.
            nn.Conv2d(512, 512, kernel_size=3, stride=1, padding=1),
            nn.BatchNorm2d(512),
            nn.ReLU(),
            nn.MaxPool2d(2, 2)
        )
        self.conv_block7 = nn.Sequential(  # Seventh convolutional block.
            nn.Conv2d(512, 1024, kernel_size=3, stride=1, padding=1),
            nn.BatchNorm2d(1024),
            nn.ReLU(),
            nn.MaxPool2d(2, 2)
        )
        self.global_avg_pool = nn.AdaptiveAvgPool2d((1, 1))  # Adaptive pooling used to handle varying input sizes.
        self.fc = nn.Sequential(
            nn.Linear(1024, 1024),  # First linear fully connected layer
            nn.ReLU(),
            nn.Dropout(0.1),  # Dropout layer allows for the neural network to learn new patterns by forgetting a
            # random number of weights. This helps to prevent overfitting and getting stuck during learning.
            nn.Linear(1024, num_classes)  # Connect last layer to outputs.
        )

    def forward(self, x):  # Pass the values through each layer in the CNN.
        x = self.conv_block1(x)
        x = self.conv_block2(x)
        x = self.conv_block3(x)
        x = self.conv_block4(x)
        x = self.conv_block5(x)
        x = self.conv_block6(x)
        x = self.conv_block7(x)
        x = self.global_avg_pool(x)
        x = x.view(x.size(0), -1)
        x = self.fc(x)
        return x


# Initialize the model, loss function, and optimizer.
# The number of classes is 102, as there are 102 different types of flowers, hence the name Flowers-102.
model = FlowerClassifier(num_classes=102).to(device)

# We will be using Cross Entropy Loss. There are many reasons for this, for example cross entropy loss encourages the
# model to assign high probabilities to the correct class and low probabilities to the incorrect classes. It is also
# very common in multi-class classification neural networks. It focuses on the overall correctness of the model
# rather than focusing on small details which is important for a dataset this size and most importantly,
# it works very well with the optimising algorithm we will be using, stochastic gradient descent.
criterion = nn.CrossEntropyLoss().to(device)

# Adam optimiser used as it allows for node based learning adjustment.
# It is considered industry standard for most applications.
optimizer = optim.Adam(model.parameters(), lr=0.0001, weight_decay=0.0075)

# ReduceLROnPlateau used to dynamically change the learning rate if model plateau's.
scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.75, patience=5)

best_val_loss = float('inf')
counter = 0
best_model_state = None


# Introduced label smoothing to prevent overfitting and provide advanced regularization.
def smooth_labels(labels, num_classes=102, smoothing_factor=0.1, device='cuda'):
    smooth_labels = torch.full(size=(labels.size(0), num_classes), fill_value=smoothing_factor / num_classes,
                               device=device)
    smooth_labels.scatter_(1, labels.unsqueeze(1), 1 - smoothing_factor + smoothing_factor / num_classes)
    return smooth_labels


for epoch in range(175):  # Train for 175 epochs
    model.train()  # Put model in training mode
    train_loss = 0
    for images, labels in train_loader:
        images, labels = images.to(device), labels.to(device)  # Send everything to gpu
        labels = smooth_labels(labels, num_classes=102, device=device)  # Apply label smoothing
        optimizer.zero_grad()  # Set gradient to 0 before processing anything
        outputs = model(images)
        loss = criterion(outputs, labels)
        loss.backward()
        optimizer.step()
        train_loss += loss.item()

    val_loss = 0
    val_accuracy = 0
    model.eval()  # Put model in evaluation mode to test val loss and accuracy.
    with torch.no_grad():
        for images, labels in val_loader:
            images, labels = images.to(device), labels.to(device)
            outputs = model(images)
            val_loss += criterion(outputs, labels).item()
            _, predicted = torch.max(outputs.data, 1)
            val_accuracy += (predicted == labels).sum().item()

    val_loss /= len(val_loader.dataset)
    val_accuracy /= len(val_loader.dataset)
    scheduler.step(val_loss)  # Scheduler steps based on validation loss

    print(
        f'Epoch {epoch + 1}, Training Loss: {train_loss / len(train_loader.dataset):.5f}, Validation Loss: {val_loss:.5f}, Validation Accuracy: {val_accuracy:.2f}')

    # Early stopping helps prevent overfitting by stopping the model if val loss stops improving.
    if val_loss < best_val_loss:  # Compare current models val loss to best one
        best_val_loss = val_loss
        best_model_state = model.state_dict()  # Save the best model's state dictionary
        counter = 0
    else:
        counter += 1
        if counter >= 20:
            print(f"Early stopping at epoch {epoch + 1}")
            break

model.load_state_dict(best_model_state)

# Save the trained model
torch.save(model.state_dict(), 'flower_classifier.pt')

# Evaluation on the test set
model.eval()  # Set the model to evaluation mode
accuracy = 0.0
with torch.no_grad():  # Disable gradient computation
    for images, labels in test_loader:
        images, labels = images.to(device), labels.to(device)
        outputs = model(images)  # Forward pass
        _, predicted = torch.max(outputs.data, 1)  # Get the predicted classes
        accuracy += (predicted == labels).sum().item()  # Calculate the accuracy

accuracy /= len(test_dataset)
print(f'Test Accuracy: {accuracy:.2f}')
